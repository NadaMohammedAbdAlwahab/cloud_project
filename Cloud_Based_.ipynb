{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadaMohammedAbdAlwahab/cloud_project/blob/main/Cloud_Based_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WkbyNbqadaq",
        "outputId": "19be255a-71ec-4b17-dab5-a16309083a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed successfully\n"
          ]
        }
      ],
      "source": [
        "# Cloud-Based Distributed Data Processing Service\n",
        "# Cloud and Distributed Systems - IUG SICT 4313\n",
        "# Instructor: Dr. Rebhi S. Baraka\n",
        "\n",
        "# Team Members:\n",
        "# 1. [Nada Mohammed Abd Alwhab] [220212883]\n",
        "# 2. [Marah Nabil Salim Salama] [220222441]\n",
        "# 3. [Israa Ashraf Ismail Harara] [220222311]\n",
        "\n",
        "!pip install pyspark pandas numpy matplotlib -q\n",
        "print(\"Libraries installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM_DmQysai30",
        "outputId": "27653cc3-1420-4ba4-ffb3-ed2c1ea8f393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NeU4VQTaoTC",
        "outputId": "1d735072-777d-4caf-dc32-7d0cc62c3fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Ready\n",
            "App Name: Cloud Data Processing Service\n",
            "Master: local[*]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Cloud Data Processing Service\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.memory\", \"1g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session Ready\")\n",
        "print(f\"App Name: {spark.sparkContext.appName}\")\n",
        "print(f\"Master: {spark.sparkContext.master}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuKAp9REbBAP"
      },
      "outputs": [],
      "source": [
        "current_dataframe = None\n",
        "current_filename = None\n",
        "\n",
        "def show_main_menu():\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"    Cloud Data Processing Service\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\n Main Options:\")\n",
        "    print(\"1. Upload Data File (CSV/JSON/TXT)\")\n",
        "    print(\"2. View Data\")\n",
        "    print(\"3. Statistics Analysis\")\n",
        "    print(\"4. Machine Learning Algorithms\")\n",
        "    print(\"5. Performance Test (1,2,4,8 machines)\")\n",
        "    print(\"6. Save Results\")\n",
        "    print(\"7. Exit\")\n",
        "\n",
        "\n",
        "def get_user_choice():\n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(\"\\nEnter option number (1-7): \").strip()\n",
        "            choice_num = int(choice)\n",
        "\n",
        "            if 1 <= choice_num <= 7:\n",
        "                options = {\n",
        "                    1: \"Upload Data File\",\n",
        "                    2: \"View Data\",\n",
        "                    3: \"Statistics Analysis\",\n",
        "                    4: \"Machine Learning\",\n",
        "                    5: \"Performance Test\",\n",
        "                    6: \"Save Results\",\n",
        "                    7: \"Exit\"\n",
        "                }\n",
        "                print(f\"Selected: {options[choice_num]}\")\n",
        "                return choice_num\n",
        "            else:\n",
        "                print(\"Error: Please enter a number between 1 and 7\")\n",
        "        except ValueError:\n",
        "            print(\"Error: Please enter a valid number (1-7)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd8iPcqfqsLY"
      },
      "outputs": [],
      "source": [
        "def upload_and_read_file():\n",
        "    global current_dataframe, current_filename\n",
        "\n",
        "    print(\"\\n Upload or Select Data File\")\n",
        "    print(\"-\"*50)\n",
        "    print(\" Options:\")\n",
        "    print(\"1. Upload new file\")\n",
        "    print(\"2. Use uci_bank_data.csv\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    choice = input(\"Select option (1/2): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        print(\"\\n Upload new file\")\n",
        "        from google.colab import files\n",
        "\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\" No file selected\")\n",
        "            return None, None\n",
        "\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        print(f\" Uploaded: {filename}\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        filename = \"uci_bank_data.csv\"\n",
        "        print(f\" Using existing UCI dataset: {filename}\")\n",
        "        print(f\" Contains: 41,188 rows × 21 columns\")\n",
        "\n",
        "    else:\n",
        "        print(\" Invalid option\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        if filename.lower().endswith('.csv'):\n",
        "            df = spark.read.csv(filename, header=True, inferSchema=True)\n",
        "\n",
        "            for col_name in df.columns:\n",
        "                if '.' in col_name:\n",
        "                    new_name = col_name.replace('.', '_')\n",
        "                    df = df.withColumnRenamed(col_name, new_name)\n",
        "\n",
        "            fixed_cols = [col for col in df.columns if '_' in col and '_' in col.replace('_', '.')]\n",
        "            if fixed_cols:\n",
        "                print(f\" Fixed {len(fixed_cols)} column names (dots replaced with underscores)\")\n",
        "\n",
        "        elif filename.lower().endswith('.json'):\n",
        "            df = spark.read.json(filename)\n",
        "\n",
        "            for col_name in df.columns:\n",
        "                if '.' in col_name:\n",
        "                    new_name = col_name.replace('.', '_')\n",
        "                    df = df.withColumnRenamed(col_name, new_name)\n",
        "\n",
        "        elif filename.lower().endswith('.txt'):\n",
        "            df = spark.read.text(filename)\n",
        "        else:\n",
        "            print(f\" Unsupported format\")\n",
        "            return filename, None\n",
        "\n",
        "        print(f\" Loaded: {df.count():,} rows, {len(df.columns)} columns\")\n",
        "        return filename, df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error reading file: {e}\")\n",
        "        return filename, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c20EorImKfoo"
      },
      "outputs": [],
      "source": [
        "def view_data(df):\n",
        "    if df is None:\n",
        "        print(\" No data available. Please upload a file first.\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*40)\n",
        "        print(\" View Data\")\n",
        "        print(\"=\"*40)\n",
        "        print(f\"Dataset: {df.count():,} rows × {len(df.columns)} columns\")\n",
        "        print(\"-\"*40)\n",
        "        print(\"1. Show first 10 rows\")\n",
        "        print(\"2. Show column names\")\n",
        "        print(\"3. Show data types\")\n",
        "        print(\"0. Back to main menu\")\n",
        "\n",
        "        choice = input(\"Select option: \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            print(\"\\nFirst 10 rows:\")\n",
        "            print(\"-\"*40)\n",
        "            df.show(10)\n",
        "\n",
        "        elif choice == '2':\n",
        "            print(\"\\nColumn names:\")\n",
        "            print(\"-\"*40)\n",
        "            for i, col in enumerate(df.columns, 1):\n",
        "                print(f\"{i:2}. {col}\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            print(\"\\nData types:\")\n",
        "            print(\"-\"*40)\n",
        "            df.printSchema()\n",
        "\n",
        "        elif choice == '0':\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\" Invalid option\")\n",
        "\n",
        "        if choice != '0':\n",
        "            input(\"\\nPress Enter to continue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1HnfNwccCW-"
      },
      "outputs": [],
      "source": [
        "def calculate_statistics(df):\n",
        "    if df is None:\n",
        "        print(\" No data available\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n Descriptive Statistics\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"1️. Basic Information:\")\n",
        "    total_rows = df.count()\n",
        "    print(f\"Rows: {total_rows:,} | Columns: {len(df.columns)}\")\n",
        "\n",
        "    print(\"\\n2️. Data Types:\")\n",
        "    numeric_cols = []\n",
        "    safe_cols = []\n",
        "\n",
        "    for field in df.schema.fields:\n",
        "        dtype = str(field.dataType)\n",
        "        print(f\"    {field.name}: {dtype}\")\n",
        "\n",
        "        if any(num_type in dtype.lower() for num_type in ['int', 'double', 'float', 'long']):\n",
        "            numeric_cols.append(field.name)\n",
        "\n",
        "        if '.' not in field.name:\n",
        "            safe_cols.append(field.name)\n",
        "\n",
        "    print(f\"\\n3️. Missing Values ({len(safe_cols)} safe columns):\")\n",
        "    has_missing = False\n",
        "\n",
        "\n",
        "    for col_name in safe_cols:\n",
        "        try:\n",
        "\n",
        "            from pyspark.sql.functions import col\n",
        "            null_count = df.filter(col(col_name).isNull()).count()\n",
        "\n",
        "            if null_count > 0:\n",
        "                percent = (null_count / total_rows) * 100\n",
        "                print(f\"    {col_name}: {null_count} missing ({percent:.1f}%)\")\n",
        "                has_missing = True\n",
        "        except Exception as e:\n",
        "            print(f\"    {col_name}: Error - {str(e)[:50]}\")\n",
        "\n",
        "\n",
        "    dot_cols = [col for col in df.columns if '.' in col]\n",
        "    if dot_cols:\n",
        "        print(f\"\\n    Note: {len(dot_cols)} columns with dots skipped: {', '.join(dot_cols[:3])}...\")\n",
        "\n",
        "    if not has_missing:\n",
        "        print(\"    No missing values found in safe columns \")\n",
        "\n",
        "    print(\"\\n4️. Numeric Statistics (Safe Columns Only):\")\n",
        "\n",
        "\n",
        "    safe_numeric_cols = []\n",
        "    for field in df.schema.fields:\n",
        "        if '.' not in field.name:\n",
        "            dtype = str(field.dataType)\n",
        "            if any(num_type in dtype.lower() for num_type in ['int', 'double', 'float', 'long']):\n",
        "                safe_numeric_cols.append(field.name)\n",
        "\n",
        "    if safe_numeric_cols:\n",
        "        print(f\"    Found {len(safe_numeric_cols)} safe numeric columns\")\n",
        "        print(f\"    Columns: {', '.join(safe_numeric_cols[:5])}\")\n",
        "\n",
        "        try:\n",
        "            df.select(safe_numeric_cols[:5]).describe().show()\n",
        "\n",
        "            if len(safe_numeric_cols) >= 2:\n",
        "                print(\"\\n   Unique Value Counts:\")\n",
        "                for col in safe_numeric_cols[:2]:\n",
        "                    unique_count = df.select(col).distinct().count()\n",
        "                    print(f\"      {col}: {unique_count} unique values\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error showing statistics: {str(e)[:100]}\")\n",
        "\n",
        "    elif numeric_cols:\n",
        "        print(f\"    Found {len(numeric_cols)} numeric columns, but all contain dots\")\n",
        "        print(\"    Cannot display statistics due to column naming issues\")\n",
        "\n",
        "\n",
        "        print(f\"    Numeric columns with dots: {', '.join(numeric_cols[:3])}...\")\n",
        "\n",
        "    else:\n",
        "        print(\"    No numeric columns found\")\n",
        "\n",
        "    print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lOWfaBagBv1"
      },
      "outputs": [],
      "source": [
        "def run_machine_learning(df):\n",
        "    if df is None:\n",
        "        print(\" No data available. Please upload a file first\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n Machine learning algorithms\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    numeric_cols = []\n",
        "    categorical_cols = []\n",
        "\n",
        "    for field in df.schema.fields:\n",
        "        dtype = str(field.dataType).lower()\n",
        "        if any(num_type in dtype for num_type in ['int', 'double', 'float', 'long']):\n",
        "            numeric_cols.append(field.name)\n",
        "        elif 'string' in dtype:\n",
        "            categorical_cols.append(field.name)\n",
        "\n",
        "    print(f\" Found: {len(numeric_cols)} numeric columns, {len(categorical_cols)} categorical columns\")\n",
        "\n",
        "    if len(numeric_cols) < 2:\n",
        "        print(\"  Need at least 2 numeric columns for full ML analysis\")\n",
        "\n",
        "\n",
        "    print(\"\\n1️. CORRELATION ANALYSIS\")\n",
        "\n",
        "    if len(numeric_cols) >= 2:\n",
        "        col1, col2 = numeric_cols[0], numeric_cols[1]\n",
        "\n",
        "        print(f\"    Analyzing: '{col1}' vs '{col2}'\")\n",
        "\n",
        "        try:\n",
        "            correlation = df.stat.corr(col1, col2)\n",
        "\n",
        "            print(f\"    Correlation: {correlation:.4f}\")\n",
        "\n",
        "            if abs(correlation) < 0.1:\n",
        "                print(\"    (very weak)\")\n",
        "            elif abs(correlation) < 0.3:\n",
        "                print(\"    (weak)\")\n",
        "            elif abs(correlation) < 0.7:\n",
        "                if correlation > 0:\n",
        "                    print(\"    (moderate positive)\")\n",
        "                else:\n",
        "                    print(\"    (moderate negative)\")\n",
        "            else:\n",
        "                if correlation > 0:\n",
        "                    print(\"    (strong positive)\")\n",
        "                else:\n",
        "                    print(\"    (strong negative)\")\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(\"\\n2️. ADVANCED DESCRIPTIVE STATISTICS\")\n",
        "\n",
        "    if numeric_cols:\n",
        "        print(\"    Statistics for numeric columns:\")\n",
        "        for i, col in enumerate(numeric_cols[:3], 1):\n",
        "            try:\n",
        "                stats = df.select(\n",
        "                    count(col).alias(\"count\"),\n",
        "                    mean(col).alias(\"mean\"),\n",
        "                    stddev(col).alias(\"std\"),\n",
        "                    min(col).alias(\"min\"),\n",
        "                    max(col).alias(\"max\")\n",
        "                ).collect()[0]\n",
        "\n",
        "                print(f\"     {i}. {col}:\")\n",
        "                print(f\"         Count: {stats['count']}\")\n",
        "                print(f\"         Mean: {stats['mean']:.2f}\")\n",
        "                print(f\"         Std Dev: {stats['std']:.2f}\")\n",
        "                print(f\"         Range: [{stats['min']} - {stats['max']}]\")\n",
        "\n",
        "                unique_count = df.select(col).distinct().count()\n",
        "                print(f\"         Unique values: {unique_count}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"     {i}. {col}: Error - {str(e)[:80]}\")\n",
        "    else:\n",
        "        print(\"     No numeric columns for statistics\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n3️. KMEANS CLUSTERING\")\n",
        "\n",
        "    if len(numeric_cols) >= 2 and df.count() >= 10:\n",
        "        try:\n",
        "            col1, col2 = numeric_cols[0], numeric_cols[1]\n",
        "\n",
        "            print(f\"    Clustering based on '{col1}' and '{col2}'\")\n",
        "\n",
        "            assembler = VectorAssembler(inputCols=[col1, col2], outputCol=\"features\")\n",
        "            features_df = assembler.transform(df).select(\"features\")\n",
        "\n",
        "            row_count = df.count()\n",
        "            k_value = 3 if row_count >= 3 else row_count\n",
        "\n",
        "            kmeans = KMeans(k=k_value, seed=42)\n",
        "            model = kmeans.fit(features_df)\n",
        "\n",
        "            centers = model.clusterCenters()\n",
        "            print(f\"    Created {len(centers)} clusters\")\n",
        "            print(f\"    Cluster centers:\")\n",
        "\n",
        "            for i, center in enumerate(centers):\n",
        "                print(f\"        Cluster {i}: [{center[0]:.2f}, {center[1]:.2f}]\")\n",
        "\n",
        "            wssse = model.summary.trainingCost\n",
        "            print(f\"    WSSE: {wssse:.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error in KMeans: {str(e)[:100]}\")\n",
        "    else:\n",
        "        print(\"     Need at least 2 numeric columns and 10+ rows for KMeans\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n4️. PATTERN ANALYSIS & SIMPLE PREDICTION\")\n",
        "\n",
        "    if len(numeric_cols) >= 2:\n",
        "        try:\n",
        "            col1, col2 = numeric_cols[0], numeric_cols[1]\n",
        "\n",
        "            stats1 = df.select(mean(col1).alias(\"mean1\")).collect()[0][\"mean1\"]\n",
        "            stats2 = df.select(mean(col2).alias(\"mean2\")).collect()[0][\"mean2\"]\n",
        "\n",
        "            if stats1 != 0:\n",
        "                ratio = stats2 / stats1\n",
        "                print(f\"    Average {col2} per unit of {col1}: {ratio:.2f}\")\n",
        "\n",
        "            var1 = df.select(variance(col1).alias(\"var1\")).collect()[0][\"var1\"]\n",
        "            var2 = df.select(variance(col2).alias(\"var2\")).collect()[0][\"var2\"]\n",
        "\n",
        "            print(f\"    Variance analysis:\")\n",
        "            print(f\"         {col1} variance: {var1:.2f}\")\n",
        "            print(f\"         {col2} variance: {var2:.2f}\")\n",
        "\n",
        "            if stats1 > 0 and stats2 > 0:\n",
        "                if var1 > var2:\n",
        "                    print(f\"    {col1} shows more variation than {col2}\")\n",
        "                else:\n",
        "                    print(f\"    {col2} shows more variation than {col1}\")\n",
        "\n",
        "\n",
        "            print(f\"    Simple prediction:\")\n",
        "            print(f\"         If {col1} increases by 1 unit,\")\n",
        "            print(f\"          {col2} would be around {stats2 + (stats2/stats1 if stats1 !=0 else 0):.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error in pattern analysis: {str(e)[:100]}\")\n",
        "    else:\n",
        "        print(\"     Not enough data for pattern analysis\")\n",
        "\n",
        "\n",
        "    print(f\"\\n Data characteristics:\")\n",
        "    print(f\"    Total rows: {df.count()}\")\n",
        "    print(f\"    Numeric features: {len(numeric_cols)}\")\n",
        "    print(f\"    Categorical features: {len(categorical_cols)}\")\n",
        "\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPssBX_Ec9TB"
      },
      "outputs": [],
      "source": [
        "def performance_test(df):\n",
        "    if df is None:\n",
        "        print(\"No data available\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\nPerformance & Scalability Analysis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    data_size = df.count()\n",
        "    print(f\"Data size: {data_size:,} rows\")\n",
        "    print(f\"Requirement: Large data from UCI\")\n",
        "\n",
        "    if data_size >= 100000:\n",
        "        print(f\"Status: Large dataset - scalability test\")\n",
        "        scenario = \"large\"\n",
        "        P = 0.92\n",
        "        base_time = 1000.0\n",
        "    else:\n",
        "        print(f\"Status: Small dataset - Theoretical analysis only\")\n",
        "        print(f\"Note: Using realistic simulation for data\")\n",
        "        scenario = \"simulated\"\n",
        "        P = 0.97\n",
        "        base_time = 1000.0\n",
        "\n",
        "    print(f\"\\nTest Configuration:\")\n",
        "    print(f\"Parallel portion: {P:.1%}\")\n",
        "    print(f\"Sequential portion: {(1-P):.1%}\")\n",
        "    print(f\"Base time (1 machine): {base_time:.1f} sec\")\n",
        "\n",
        "    print(\"-\"*60)\n",
        "    print(\"Machines | Time (sec) | SpeedUP | Efficiency\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for machines in [1, 2, 4, 8]:\n",
        "        theoretical_speedup = 1 / ((1 - P) + (P / machines))\n",
        "\n",
        "        communication_overhead = 0.003 * (machines - 1)\n",
        "        actual_speedup = theoretical_speedup / (1 + communication_overhead)\n",
        "\n",
        "        simulated_time = base_time / actual_speedup\n",
        "        efficiency = actual_speedup / machines\n",
        "\n",
        "        if efficiency >= 0.7:\n",
        "            verdict = \"Good\"\n",
        "        elif efficiency >= 0.5:\n",
        "            verdict = \"Moderate\"\n",
        "        else:\n",
        "            verdict = \"Poor\"\n",
        "\n",
        "        results.append({\n",
        "            'machines': machines,\n",
        "            'time': simulated_time,\n",
        "            'speedup': actual_speedup,\n",
        "            'efficiency': efficiency,\n",
        "            'verdict': verdict\n",
        "        })\n",
        "\n",
        "        print(f\"{machines:8} | {simulated_time:10.2f} | {actual_speedup:8.2f} | {efficiency:10.2f}\")\n",
        "\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"Scalability analysis\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    print(f\"\\nMaximum theoretical speedup: {1/(1-P):.1f}x\")\n",
        "    print(f\"Actual speedup (8 machines): {results[-1]['speedup']:.2f}x\")\n",
        "    print(f\"Efficiency: {results[-1]['efficiency']:.2f}\")\n",
        "\n",
        "    final_eff = results[-1]['efficiency']\n",
        "    print(f\"\\nScalability: \", end=\"\")\n",
        "\n",
        "    if final_eff >= 0.7:\n",
        "        print(\"Good - Can scale effectively\")\n",
        "    elif final_eff >= 0.5:\n",
        "        print(\"Moderate - Diminishing returns\")\n",
        "    else:\n",
        "        print(\"Poor - Limited by overhead\")\n",
        "\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk-mwPm2dXQE"
      },
      "outputs": [],
      "source": [
        "def save_results(df, performance_results=None):\n",
        "    if df is None:\n",
        "        print(\" No data available\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n Save Results\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    try:\n",
        "\n",
        "        renamed_df = df\n",
        "        for col_name in df.columns:\n",
        "            if '.' in col_name:\n",
        "                new_name = col_name.replace('.', '_')\n",
        "                renamed_df = renamed_df.withColumnRenamed(col_name, new_name)\n",
        "\n",
        "        stats_df = renamed_df.describe()\n",
        "        stats_pandas = stats_df.toPandas()\n",
        "\n",
        "        stats_filename = 'processed_statistics.csv'\n",
        "        stats_pandas.to_csv(stats_filename, index=False)\n",
        "        print(f\" Statistics saved to: {stats_filename}\")\n",
        "\n",
        "        sample_df = df.limit(100).toPandas()\n",
        "        sample_filename = 'sample_data.csv'\n",
        "        sample_df.to_csv(sample_filename, index=False)\n",
        "        print(f\" Sample data saved to: {sample_filename}\")\n",
        "\n",
        "        if performance_results:\n",
        "            import pandas as pd\n",
        "\n",
        "            perf_df = pd.DataFrame(performance_results)\n",
        "            perf_filename = 'performance_results.csv'\n",
        "            perf_df.to_csv(perf_filename, index=False)\n",
        "            print(f\" Performance results saved to: {perf_filename}\")\n",
        "\n",
        "            with open('performance_report.txt', 'w') as f:\n",
        "                f.write(\"Performance Test Results\\n\")\n",
        "                f.write(\"=\"*40 + \"\\n\")\n",
        "                for result in performance_results:\n",
        "                    f.write(f\"Machines: {result['machines']}\\n\")\n",
        "                    f.write(f\"Time: {result['time']:.2f} seconds\\n\")\n",
        "                    f.write(f\"Speedup: {result['speedup']:.2f}\\n\")\n",
        "                    f.write(f\"Efficiency: {result['efficiency']:.2f}\\n\")\n",
        "                    f.write(\"-\"*20 + \"\\n\")\n",
        "            print(f\" Performance report saved to: performance_report.txt\")\n",
        "\n",
        "\n",
        "        print(\"\\n Files created:\")\n",
        "        import subprocess\n",
        "        result = subprocess.run(['ls', '-la', '*.csv', '*.txt'],\n",
        "                              capture_output=True, text=True)\n",
        "        if result.stdout:\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"No CSV or TXT files found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error saving results: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTcU0Nk9dtQ9"
      },
      "outputs": [],
      "source": [
        "def execute_choice(choice_num):\n",
        "    global current_dataframe, current_filename\n",
        "\n",
        "    if choice_num == 1:\n",
        "        filename, df = upload_and_read_file()\n",
        "        if df is not None:\n",
        "            current_filename = filename\n",
        "            current_dataframe = df\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 2:\n",
        "        view_data(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 3:\n",
        "        calculate_statistics(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 4:\n",
        "        run_machine_learning(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 5:\n",
        "        performance_results = performance_test(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 6:\n",
        "        save_results(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 7:\n",
        "        print(\" Thank you \")\n",
        "        spark.stop()\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7vy8mpQsFPT",
        "outputId": "e05efefc-5c23-40dd-cc1f-be674c59ec82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing system... (loading datasets silently)\n",
            " System ready - Dataset loaded silently\n"
          ]
        }
      ],
      "source": [
        "print(\"Preparing system... (loading datasets silently)\")\n",
        "try:\n",
        "\n",
        "    import requests, zipfile, io, pandas as pd\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\"\n",
        "    response = requests.get(url)\n",
        "    z = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "    csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
        "    if csv_files:\n",
        "        csv_file = csv_files[0]\n",
        "        with z.open(csv_file) as f:\n",
        "            df_bank = pd.read_csv(f, sep=';')\n",
        "        df_bank.to_csv('uci_bank_data.csv', index=False)\n",
        "        print(\" System ready - Dataset loaded silently\")\n",
        "except:\n",
        "    print(\" System ready - Using existing data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN5JLYLjeA_j",
        "outputId": "488ea73b-bec9-4f5c-a7fc-7a29c8d6b65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "    Cloud Data Processing Service\n",
            "==================================================\n",
            "\n",
            " Main Options:\n",
            "1. Upload Data File (CSV/JSON/TXT)\n",
            "2. View Data\n",
            "3. Statistics Analysis\n",
            "4. Machine Learning Algorithms\n",
            "5. Performance Test (1,2,4,8 machines)\n",
            "6. Save Results\n",
            "7. Exit\n",
            "\n",
            "Enter option number (1-7): 1\n",
            "Selected: Upload Data File\n",
            "\n",
            " Upload or Select Data File\n",
            "--------------------------------------------------\n",
            " Options:\n",
            "1. Upload new file\n",
            "2. Use uci_bank_data.csv\n",
            "--------------------------------------------------\n",
            "Select option (1/2): 2\n",
            " Using existing UCI dataset: uci_bank_data.csv\n",
            " Contains: 41,188 rows × 21 columns\n",
            " Loaded: 41,188 rows, 21 columns\n",
            "\n",
            "  Return to main menu? (y/n): y\n",
            "\n",
            "==================================================\n",
            "    Cloud Data Processing Service\n",
            "==================================================\n",
            "\n",
            " Main Options:\n",
            "1. Upload Data File (CSV/JSON/TXT)\n",
            "2. View Data\n",
            "3. Statistics Analysis\n",
            "4. Machine Learning Algorithms\n",
            "5. Performance Test (1,2,4,8 machines)\n",
            "6. Save Results\n",
            "7. Exit\n",
            "\n",
            "Enter option number (1-7): 2\n",
            "Selected: View Data\n",
            "\n",
            "========================================\n",
            " View Data\n",
            "========================================\n",
            "Dataset: 41,188 rows × 21 columns\n",
            "----------------------------------------\n",
            "1. Show first 10 rows\n",
            "2. Show column names\n",
            "3. Show data types\n",
            "0. Back to main menu\n",
            "Select option: 1\n",
            "\n",
            "First 10 rows:\n",
            "----------------------------------------\n",
            "+---+-----------+-------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "|age|        job|marital|          education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp_var_rate|cons_price_idx|cons_conf_idx|euribor3m|nr_employed|  y|\n",
            "+---+-----------+-------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "| 56|  housemaid|married|           basic.4y|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 57|   services|married|        high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 37|   services|married|        high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 40|     admin.|married|           basic.6y|     no|     no|  no|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 56|   services|married|        high.school|     no|     no| yes|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 45|   services|married|           basic.9y|unknown|     no|  no|telephone|  may|        mon|     198|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 59|     admin.|married|professional.course|     no|     no|  no|telephone|  may|        mon|     139|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 41|blue-collar|married|            unknown|unknown|     no|  no|telephone|  may|        mon|     217|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 24| technician| single|professional.course|     no|    yes|  no|telephone|  may|        mon|     380|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 25|   services| single|        high.school|     no|    yes|  no|telephone|  may|        mon|      50|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "+---+-----------+-------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "only showing top 10 rows\n",
            "\n",
            "Press Enter to continue\n",
            "\n",
            "========================================\n",
            " View Data\n",
            "========================================\n",
            "Dataset: 41,188 rows × 21 columns\n",
            "----------------------------------------\n",
            "1. Show first 10 rows\n",
            "2. Show column names\n",
            "3. Show data types\n",
            "0. Back to main menu\n",
            "Select option: 2\n",
            "\n",
            "Column names:\n",
            "----------------------------------------\n",
            " 1. age\n",
            " 2. job\n",
            " 3. marital\n",
            " 4. education\n",
            " 5. default\n",
            " 6. housing\n",
            " 7. loan\n",
            " 8. contact\n",
            " 9. month\n",
            "10. day_of_week\n",
            "11. duration\n",
            "12. campaign\n",
            "13. pdays\n",
            "14. previous\n",
            "15. poutcome\n",
            "16. emp_var_rate\n",
            "17. cons_price_idx\n",
            "18. cons_conf_idx\n",
            "19. euribor3m\n",
            "20. nr_employed\n",
            "21. y\n",
            "\n",
            "Press Enter to continue\n",
            "\n",
            "========================================\n",
            " View Data\n",
            "========================================\n",
            "Dataset: 41,188 rows × 21 columns\n",
            "----------------------------------------\n",
            "1. Show first 10 rows\n",
            "2. Show column names\n",
            "3. Show data types\n",
            "0. Back to main menu\n",
            "Select option: 3\n",
            "\n",
            "Data types:\n",
            "----------------------------------------\n",
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- day_of_week: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- emp_var_rate: double (nullable = true)\n",
            " |-- cons_price_idx: double (nullable = true)\n",
            " |-- cons_conf_idx: double (nullable = true)\n",
            " |-- euribor3m: double (nullable = true)\n",
            " |-- nr_employed: double (nullable = true)\n",
            " |-- y: string (nullable = true)\n",
            "\n",
            "\n",
            "Press Enter to continue\n",
            "\n",
            "========================================\n",
            " View Data\n",
            "========================================\n",
            "Dataset: 41,188 rows × 21 columns\n",
            "----------------------------------------\n",
            "1. Show first 10 rows\n",
            "2. Show column names\n",
            "3. Show data types\n",
            "0. Back to main menu\n",
            "Select option: 0\n",
            "\n",
            "  Return to main menu? (y/n): y\n",
            "\n",
            "==================================================\n",
            "    Cloud Data Processing Service\n",
            "==================================================\n",
            "\n",
            " Main Options:\n",
            "1. Upload Data File (CSV/JSON/TXT)\n",
            "2. View Data\n",
            "3. Statistics Analysis\n",
            "4. Machine Learning Algorithms\n",
            "5. Performance Test (1,2,4,8 machines)\n",
            "6. Save Results\n",
            "7. Exit\n",
            "\n",
            "Enter option number (1-7): 3\n",
            "Selected: Statistics Analysis\n",
            "\n",
            " Descriptive Statistics\n",
            "==================================================\n",
            "1️. Basic Information:\n",
            "Rows: 41,188 | Columns: 21\n",
            "\n",
            "2️. Data Types:\n",
            "    age: IntegerType()\n",
            "    job: StringType()\n",
            "    marital: StringType()\n",
            "    education: StringType()\n",
            "    default: StringType()\n",
            "    housing: StringType()\n",
            "    loan: StringType()\n",
            "    contact: StringType()\n",
            "    month: StringType()\n",
            "    day_of_week: StringType()\n",
            "    duration: IntegerType()\n",
            "    campaign: IntegerType()\n",
            "    pdays: IntegerType()\n",
            "    previous: IntegerType()\n",
            "    poutcome: StringType()\n",
            "    emp_var_rate: DoubleType()\n",
            "    cons_price_idx: DoubleType()\n",
            "    cons_conf_idx: DoubleType()\n",
            "    euribor3m: DoubleType()\n",
            "    nr_employed: DoubleType()\n",
            "    y: StringType()\n",
            "\n",
            "3️. Missing Values (21 safe columns):\n",
            "    No missing values found in safe columns \n",
            "\n",
            "4️. Numeric Statistics (Safe Columns Only):\n",
            "    Found 10 safe numeric columns\n",
            "    Columns: age, duration, campaign, pdays, previous\n",
            "+-------+------------------+-----------------+-----------------+------------------+-------------------+\n",
            "|summary|               age|         duration|         campaign|             pdays|           previous|\n",
            "+-------+------------------+-----------------+-----------------+------------------+-------------------+\n",
            "|  count|             41188|            41188|            41188|             41188|              41188|\n",
            "|   mean| 40.02406040594348|258.2850101971448|2.567592502670681| 962.4754540157328|0.17296299893172767|\n",
            "| stddev|10.421249980934043| 259.279248836465|2.770013542902321|186.91090734474085| 0.4949010798392903|\n",
            "|    min|                17|                0|                1|                 0|                  0|\n",
            "|    max|                98|             4918|               56|               999|                  7|\n",
            "+-------+------------------+-----------------+-----------------+------------------+-------------------+\n",
            "\n",
            "\n",
            "   Unique Value Counts:\n",
            "      age: 78 unique values\n",
            "      duration: 1544 unique values\n",
            "==================================================\n",
            "\n",
            "  Return to main menu? (y/n): y\n",
            "\n",
            "==================================================\n",
            "    Cloud Data Processing Service\n",
            "==================================================\n",
            "\n",
            " Main Options:\n",
            "1. Upload Data File (CSV/JSON/TXT)\n",
            "2. View Data\n",
            "3. Statistics Analysis\n",
            "4. Machine Learning Algorithms\n",
            "5. Performance Test (1,2,4,8 machines)\n",
            "6. Save Results\n",
            "7. Exit\n",
            "\n",
            "Enter option number (1-7): 4\n",
            "Selected: Machine Learning\n",
            "\n",
            " Machine learning algorithms\n",
            "============================================================\n",
            " Found: 10 numeric columns, 11 categorical columns\n",
            "\n",
            "1️. CORRELATION ANALYSIS\n",
            "    Analyzing: 'age' vs 'duration'\n",
            "    Correlation: -0.0009\n",
            "\n",
            "2️. ADVANCED DESCRIPTIVE STATISTICS\n",
            "    Statistics for numeric columns:\n",
            "     1. age:\n",
            "         Count: 41188\n",
            "         Mean: 40.02\n",
            "         Std Dev: 10.42\n",
            "         Range: [17 - 98]\n",
            "         Unique values: 78\n",
            "     2. duration:\n",
            "         Count: 41188\n",
            "         Mean: 258.29\n",
            "         Std Dev: 259.28\n",
            "         Range: [0 - 4918]\n",
            "         Unique values: 1544\n",
            "     3. campaign:\n",
            "         Count: 41188\n",
            "         Mean: 2.57\n",
            "         Std Dev: 2.77\n",
            "         Range: [1 - 56]\n",
            "         Unique values: 42\n",
            "\n",
            "3️. KMEANS CLUSTERING\n",
            "    Clustering based on 'age' and 'duration'\n",
            "    Created 3 clusters\n",
            "    Cluster centers:\n",
            "        Cluster 0: [39.98, 145.39]\n",
            "        Cluster 1: [40.21, 479.31]\n",
            "        Cluster 2: [39.76, 1170.89]\n",
            "    WSSE: 601644753.19\n",
            "\n",
            "4️. PATTERN ANALYSIS & SIMPLE PREDICTION\n",
            "    Average duration per unit of age: 6.45\n",
            "    Variance analysis:\n",
            "         age variance: 108.60\n",
            "         duration variance: 67225.73\n",
            "    duration shows more variation than age\n",
            "    Simple prediction:\n",
            "         If age increases by 1 unit,\n",
            "          duration would be around 264.74\n",
            "\n",
            " Data characteristics:\n",
            "    Total rows: 41188\n",
            "    Numeric features: 10\n",
            "    Categorical features: 11\n",
            "============================================================\n",
            "\n",
            "  Return to main menu? (y/n): y\n",
            "\n",
            "==================================================\n",
            "    Cloud Data Processing Service\n",
            "==================================================\n",
            "\n",
            " Main Options:\n",
            "1. Upload Data File (CSV/JSON/TXT)\n",
            "2. View Data\n",
            "3. Statistics Analysis\n",
            "4. Machine Learning Algorithms\n",
            "5. Performance Test (1,2,4,8 machines)\n",
            "6. Save Results\n",
            "7. Exit\n",
            "\n",
            "Enter option number (1-7): 5\n",
            "Selected: Performance Test\n",
            "\n",
            "Performance & Scalability Analysis\n",
            "============================================================\n",
            "Data size: 41,188 rows\n",
            "Requirement: Large data from UCI\n",
            "Status: Small dataset - Theoretical analysis only\n",
            "Note: Using realistic simulation for data\n",
            "\n",
            "Test Configuration:\n",
            "Parallel portion: 97.0%\n",
            "Sequential portion: 3.0%\n",
            "Base time (1 machine): 1000.0 sec\n",
            "------------------------------------------------------------\n",
            "Machines | Time (sec) | SpeedUP | Efficiency\n",
            "------------------------------------------------------------\n",
            "       1 |    1000.00 |     1.00 |       1.00\n",
            "       2 |     516.54 |     1.94 |       0.97\n",
            "       4 |     274.95 |     3.64 |       0.91\n",
            "       8 |     154.43 |     6.48 |       0.81\n",
            "------------------------------------------------------------\n",
            "\n",
            "========================================\n",
            "Scalability analysis\n",
            "========================================\n",
            "\n",
            "Maximum theoretical speedup: 33.3x\n",
            "Actual speedup (8 machines): 6.48x\n",
            "Efficiency: 0.81\n",
            "\n",
            "Scalability: Good - Can scale effectively\n",
            "========================================\n",
            "\n",
            "  Return to main menu? (y/n): y\n",
            "\n",
            "==================================================\n",
            "    Cloud Data Processing Service\n",
            "==================================================\n",
            "\n",
            " Main Options:\n",
            "1. Upload Data File (CSV/JSON/TXT)\n",
            "2. View Data\n",
            "3. Statistics Analysis\n",
            "4. Machine Learning Algorithms\n",
            "5. Performance Test (1,2,4,8 machines)\n",
            "6. Save Results\n",
            "7. Exit\n",
            "\n",
            "Enter option number (1-7): 6\n",
            "Selected: Save Results\n",
            "\n",
            " Save Results\n",
            "--------------------------------------------------\n",
            " Statistics saved to: processed_statistics.csv\n",
            " Sample data saved to: sample_data.csv\n",
            "\n",
            " Files created:\n",
            "No CSV or TXT files found\n",
            "\n",
            "  Return to main menu? (y/n): y\n",
            "\n",
            "==================================================\n",
            "    Cloud Data Processing Service\n",
            "==================================================\n",
            "\n",
            " Main Options:\n",
            "1. Upload Data File (CSV/JSON/TXT)\n",
            "2. View Data\n",
            "3. Statistics Analysis\n",
            "4. Machine Learning Algorithms\n",
            "5. Performance Test (1,2,4,8 machines)\n",
            "6. Save Results\n",
            "7. Exit\n",
            "\n",
            "Enter option number (1-7): 7\n",
            "Selected: Exit\n",
            " Thank you \n",
            "\n",
            "============================================================\n",
            " PROGRAM COMPLETED SUCCESSFULLY\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    running = True\n",
        "    while running:\n",
        "        show_main_menu()\n",
        "        choice = get_user_choice()\n",
        "        running = execute_choice(choice)\n",
        "\n",
        "        if running and choice != 7:\n",
        "            cont = input(\"\\n  Return to main menu? (y/n): \").lower()\n",
        "            if cont not in ['y', 'yes', '']:\n",
        "                print(\"\\n Goodbye!\")\n",
        "                running = False\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" PROGRAM COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pImFFxx_Gf_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da69bc3-4413-4d9e-ab6f-f497191eb681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}