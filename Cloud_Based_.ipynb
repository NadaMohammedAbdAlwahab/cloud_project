{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadaMohammedAbdAlwahab/cloud_project/blob/main/Cloud_Based_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WkbyNbqadaq",
        "outputId": "06479961-2869-4eaf-abc9-bb342620b690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed successfully\n"
          ]
        }
      ],
      "source": [
        "# Cloud-Based Distributed Data Processing Service\n",
        "# Cloud and Distributed Systems - IUG SICT 4313\n",
        "# Instructor: Dr. Rebhi S. Baraka\n",
        "\n",
        "# Team Members:\n",
        "# 1. [Nada Mohammed Abd Alwhab] [220212883]\n",
        "# 2. [Marah Nabil Salim Salama] [220222441]\n",
        "# 3. [Israa Ashraf Ismail Harara] [220222311]\n",
        "\n",
        "!pip install pyspark pandas numpy matplotlib -q\n",
        "print(\"Libraries installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM_DmQysai30",
        "outputId": "b843b63a-88d3-4748-f25e-868c1396f198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NeU4VQTaoTC",
        "outputId": "7ad51887-0443-4482-9a07-30881ef5ea89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Ready\n",
            "App Name: Cloud Data Processing Service\n",
            "Master: local[*]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Cloud Data Processing Service\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.memory\", \"1g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session Ready\")\n",
        "print(f\"App Name: {spark.sparkContext.appName}\")\n",
        "print(f\"Master: {spark.sparkContext.master}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuKAp9REbBAP"
      },
      "outputs": [],
      "source": [
        "current_dataframe = None\n",
        "current_filename = None\n",
        "\n",
        "def show_main_menu():\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"    Cloud Data Processing Service\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\n Main Options:\")\n",
        "    print(\"1. Upload Data File (CSV/JSON/TXT)\")\n",
        "    print(\"2. View Data\")\n",
        "    print(\"3. Statistics Analysis\")\n",
        "    print(\"4. Machine Learning Algorithms\")\n",
        "    print(\"5. Performance Test (1,2,4,8 machines)\")\n",
        "    print(\"6. Save Results\")\n",
        "    print(\"7. Exit\")\n",
        "\n",
        "\n",
        "def get_user_choice():\n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(\"\\nEnter option number (1-7): \").strip()\n",
        "            choice_num = int(choice)\n",
        "\n",
        "            if 1 <= choice_num <= 7:\n",
        "                options = {\n",
        "                    1: \"Upload Data File\",\n",
        "                    2: \"View Data\",\n",
        "                    3: \"Statistics Analysis\",\n",
        "                    4: \"Machine Learning\",\n",
        "                    5: \"Performance Test\",\n",
        "                    6: \"Save Results\",\n",
        "                    7: \"Exit\"\n",
        "                }\n",
        "                print(f\"Selected: {options[choice_num]}\")\n",
        "                return choice_num\n",
        "            else:\n",
        "                print(\"Error: Please enter a number between 1 and 7\")\n",
        "        except ValueError:\n",
        "            print(\"Error: Please enter a valid number (1-7)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd8iPcqfqsLY"
      },
      "outputs": [],
      "source": [
        "def upload_and_read_file():\n",
        "    global current_dataframe, current_filename\n",
        "\n",
        "    print(\"\\n Upload or Select Data File\")\n",
        "    print(\"-\"*50)\n",
        "    print(\" Options:\")\n",
        "    print(\"1. Upload new file\")\n",
        "    print(\"2. Use uci_bank_data.csv\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    choice = input(\"Select option (1/2): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        print(\"\\n Upload new file\")\n",
        "        from google.colab import files\n",
        "\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\" No file selected\")\n",
        "            return None, None\n",
        "\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        print(f\" Uploaded: {filename}\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        filename = \"uci_bank_data.csv\"\n",
        "        print(f\" Using existing UCI dataset: {filename}\")\n",
        "        print(f\" Contains: 41,188 rows × 21 columns\")\n",
        "\n",
        "    else:\n",
        "        print(\" Invalid option\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        if filename.lower().endswith('.csv'):\n",
        "            df = spark.read.csv(filename, header=True, inferSchema=True)\n",
        "\n",
        "            for col_name in df.columns:\n",
        "                if '.' in col_name:\n",
        "                    new_name = col_name.replace('.', '_')\n",
        "                    df = df.withColumnRenamed(col_name, new_name)\n",
        "\n",
        "            fixed_cols = [col for col in df.columns if '_' in col and '_' in col.replace('_', '.')]\n",
        "            if fixed_cols:\n",
        "                print(f\" Fixed {len(fixed_cols)} column names (dots replaced with underscores)\")\n",
        "\n",
        "        elif filename.lower().endswith('.json'):\n",
        "            df = spark.read.json(filename)\n",
        "\n",
        "            for col_name in df.columns:\n",
        "                if '.' in col_name:\n",
        "                    new_name = col_name.replace('.', '_')\n",
        "                    df = df.withColumnRenamed(col_name, new_name)\n",
        "\n",
        "        elif filename.lower().endswith('.txt'):\n",
        "            df = spark.read.text(filename)\n",
        "        else:\n",
        "            print(f\" Unsupported format\")\n",
        "            return filename, None\n",
        "\n",
        "        print(f\" Loaded: {df.count():,} rows, {len(df.columns)} columns\")\n",
        "        return filename, df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error reading file: {e}\")\n",
        "        return filename, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c20EorImKfoo"
      },
      "outputs": [],
      "source": [
        "def view_data(df):\n",
        "    if df is None:\n",
        "        print(\" No data available. Please upload a file first.\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*40)\n",
        "        print(\" View Data\")\n",
        "        print(\"=\"*40)\n",
        "        print(f\"Dataset: {df.count():,} rows × {len(df.columns)} columns\")\n",
        "        print(\"-\"*40)\n",
        "        print(\"1. Show first 10 rows\")\n",
        "        print(\"2. Show column names\")\n",
        "        print(\"3. Show data types\")\n",
        "        print(\"0. Back to main menu\")\n",
        "\n",
        "        choice = input(\"Select option: \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            print(\"\\nFirst 10 rows:\")\n",
        "            print(\"-\"*40)\n",
        "            df.show(10)\n",
        "\n",
        "        elif choice == '2':\n",
        "            print(\"\\nColumn names:\")\n",
        "            print(\"-\"*40)\n",
        "            for i, col in enumerate(df.columns, 1):\n",
        "                print(f\"{i:2}. {col}\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            print(\"\\nData types:\")\n",
        "            print(\"-\"*40)\n",
        "            df.printSchema()\n",
        "\n",
        "        elif choice == '0':\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\" Invalid option\")\n",
        "\n",
        "        if choice != '0':\n",
        "            input(\"\\nPress Enter to continue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1HnfNwccCW-"
      },
      "outputs": [],
      "source": [
        "def calculate_statistics(df):\n",
        "    if df is None:\n",
        "        print(\" No data available\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n Descriptive Statistics\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"1️. Basic Information:\")\n",
        "    total_rows = df.count()\n",
        "    print(f\"Rows: {total_rows:,} | Columns: {len(df.columns)}\")\n",
        "\n",
        "    print(\"\\n2️. Data Types:\")\n",
        "    numeric_cols = []\n",
        "    safe_cols = []\n",
        "\n",
        "    for field in df.schema.fields:\n",
        "        dtype = str(field.dataType)\n",
        "        print(f\"    {field.name}: {dtype}\")\n",
        "\n",
        "        if any(num_type in dtype.lower() for num_type in ['int', 'double', 'float', 'long']):\n",
        "            numeric_cols.append(field.name)\n",
        "\n",
        "        if '.' not in field.name:\n",
        "            safe_cols.append(field.name)\n",
        "\n",
        "    print(f\"\\n3️. Missing Values ({len(safe_cols)} safe columns):\")\n",
        "    has_missing = False\n",
        "\n",
        "\n",
        "    for col_name in safe_cols:\n",
        "        try:\n",
        "\n",
        "            from pyspark.sql.functions import col\n",
        "            null_count = df.filter(col(col_name).isNull()).count()\n",
        "\n",
        "            if null_count > 0:\n",
        "                percent = (null_count / total_rows) * 100\n",
        "                print(f\"    {col_name}: {null_count} missing ({percent:.1f}%)\")\n",
        "                has_missing = True\n",
        "        except Exception as e:\n",
        "            print(f\"    {col_name}: Error - {str(e)[:50]}\")\n",
        "\n",
        "\n",
        "    dot_cols = [col for col in df.columns if '.' in col]\n",
        "    if dot_cols:\n",
        "        print(f\"\\n    Note: {len(dot_cols)} columns with dots skipped: {', '.join(dot_cols[:3])}...\")\n",
        "\n",
        "    if not has_missing:\n",
        "        print(\"    No missing values found in safe columns \")\n",
        "\n",
        "    print(\"\\n4️. Numeric Statistics (Safe Columns Only):\")\n",
        "\n",
        "\n",
        "    safe_numeric_cols = []\n",
        "    for field in df.schema.fields:\n",
        "        if '.' not in field.name:\n",
        "            dtype = str(field.dataType)\n",
        "            if any(num_type in dtype.lower() for num_type in ['int', 'double', 'float', 'long']):\n",
        "                safe_numeric_cols.append(field.name)\n",
        "\n",
        "    if safe_numeric_cols:\n",
        "        print(f\"    Found {len(safe_numeric_cols)} safe numeric columns\")\n",
        "        print(f\"    Columns: {', '.join(safe_numeric_cols[:5])}\")\n",
        "\n",
        "        try:\n",
        "            df.select(safe_numeric_cols[:5]).describe().show()\n",
        "\n",
        "            if len(safe_numeric_cols) >= 2:\n",
        "                print(\"\\n   Unique Value Counts:\")\n",
        "                for col in safe_numeric_cols[:2]:\n",
        "                    unique_count = df.select(col).distinct().count()\n",
        "                    print(f\"      {col}: {unique_count} unique values\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error showing statistics: {str(e)[:100]}\")\n",
        "\n",
        "    elif numeric_cols:\n",
        "        print(f\"    Found {len(numeric_cols)} numeric columns, but all contain dots\")\n",
        "        print(\"    Cannot display statistics due to column naming issues\")\n",
        "\n",
        "\n",
        "        print(f\"    Numeric columns with dots: {', '.join(numeric_cols[:3])}...\")\n",
        "\n",
        "    else:\n",
        "        print(\"    No numeric columns found\")\n",
        "\n",
        "    print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lOWfaBagBv1"
      },
      "outputs": [],
      "source": [
        "def run_machine_learning(df):\n",
        "    if df is None:\n",
        "        print(\" No data available. Please upload a file first\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n Machine learning algorithms\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    numeric_cols = []\n",
        "    categorical_cols = []\n",
        "\n",
        "    for field in df.schema.fields:\n",
        "        dtype = str(field.dataType).lower()\n",
        "        if any(num_type in dtype for num_type in ['int', 'double', 'float', 'long']):\n",
        "            numeric_cols.append(field.name)\n",
        "        elif 'string' in dtype:\n",
        "            categorical_cols.append(field.name)\n",
        "\n",
        "    print(f\" Found: {len(numeric_cols)} numeric columns, {len(categorical_cols)} categorical columns\")\n",
        "\n",
        "    if len(numeric_cols) < 2:\n",
        "        print(\"  Need at least 2 numeric columns for full ML analysis\")\n",
        "\n",
        "\n",
        "    print(\"\\n1️. CORRELATION ANALYSIS\")\n",
        "\n",
        "    if len(numeric_cols) >= 2:\n",
        "        col1, col2 = numeric_cols[0], numeric_cols[1]\n",
        "\n",
        "        print(f\"    Analyzing: '{col1}' vs '{col2}'\")\n",
        "\n",
        "        try:\n",
        "            correlation = df.stat.corr(col1, col2)\n",
        "\n",
        "            print(f\"    Correlation: {correlation:.4f}\")\n",
        "\n",
        "            if abs(correlation) < 0.1:\n",
        "                print(\"    (very weak)\")\n",
        "            elif abs(correlation) < 0.3:\n",
        "                print(\"    (weak)\")\n",
        "            elif abs(correlation) < 0.7:\n",
        "                if correlation > 0:\n",
        "                    print(\"    (moderate positive)\")\n",
        "                else:\n",
        "                    print(\"    (moderate negative)\")\n",
        "            else:\n",
        "                if correlation > 0:\n",
        "                    print(\"    (strong positive)\")\n",
        "                else:\n",
        "                    print(\"    (strong negative)\")\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(\"\\n2️. ADVANCED DESCRIPTIVE STATISTICS\")\n",
        "\n",
        "    if numeric_cols:\n",
        "        print(\"    Statistics for numeric columns:\")\n",
        "        for i, col in enumerate(numeric_cols[:3], 1):\n",
        "            try:\n",
        "                stats = df.select(\n",
        "                    count(col).alias(\"count\"),\n",
        "                    mean(col).alias(\"mean\"),\n",
        "                    stddev(col).alias(\"std\"),\n",
        "                    min(col).alias(\"min\"),\n",
        "                    max(col).alias(\"max\")\n",
        "                ).collect()[0]\n",
        "\n",
        "                print(f\"     {i}. {col}:\")\n",
        "                print(f\"         Count: {stats['count']}\")\n",
        "                print(f\"         Mean: {stats['mean']:.2f}\")\n",
        "                print(f\"         Std Dev: {stats['std']:.2f}\")\n",
        "                print(f\"         Range: [{stats['min']} - {stats['max']}]\")\n",
        "\n",
        "                unique_count = df.select(col).distinct().count()\n",
        "                print(f\"         Unique values: {unique_count}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"     {i}. {col}: Error - {str(e)[:80]}\")\n",
        "    else:\n",
        "        print(\"     No numeric columns for statistics\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n3️. KMEANS CLUSTERING\")\n",
        "\n",
        "    if len(numeric_cols) >= 2 and df.count() >= 10:\n",
        "        try:\n",
        "            col1, col2 = numeric_cols[0], numeric_cols[1]\n",
        "\n",
        "            print(f\"    Clustering based on '{col1}' and '{col2}'\")\n",
        "\n",
        "            assembler = VectorAssembler(inputCols=[col1, col2], outputCol=\"features\")\n",
        "            features_df = assembler.transform(df).select(\"features\")\n",
        "\n",
        "            row_count = df.count()\n",
        "            k_value = 3 if row_count >= 3 else row_count\n",
        "\n",
        "            kmeans = KMeans(k=k_value, seed=42)\n",
        "            model = kmeans.fit(features_df)\n",
        "\n",
        "            centers = model.clusterCenters()\n",
        "            print(f\"    Created {len(centers)} clusters\")\n",
        "            print(f\"    Cluster centers:\")\n",
        "\n",
        "            for i, center in enumerate(centers):\n",
        "                print(f\"        Cluster {i}: [{center[0]:.2f}, {center[1]:.2f}]\")\n",
        "\n",
        "            wssse = model.summary.trainingCost\n",
        "            print(f\"    WSSE: {wssse:.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error in KMeans: {str(e)[:100]}\")\n",
        "    else:\n",
        "        print(\"     Need at least 2 numeric columns and 10+ rows for KMeans\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n4️. PATTERN ANALYSIS & SIMPLE PREDICTION\")\n",
        "\n",
        "    if len(numeric_cols) >= 2:\n",
        "        try:\n",
        "            col1, col2 = numeric_cols[0], numeric_cols[1]\n",
        "\n",
        "            stats1 = df.select(mean(col1).alias(\"mean1\")).collect()[0][\"mean1\"]\n",
        "            stats2 = df.select(mean(col2).alias(\"mean2\")).collect()[0][\"mean2\"]\n",
        "\n",
        "            if stats1 != 0:\n",
        "                ratio = stats2 / stats1\n",
        "                print(f\"    Average {col2} per unit of {col1}: {ratio:.2f}\")\n",
        "\n",
        "            var1 = df.select(variance(col1).alias(\"var1\")).collect()[0][\"var1\"]\n",
        "            var2 = df.select(variance(col2).alias(\"var2\")).collect()[0][\"var2\"]\n",
        "\n",
        "            print(f\"    Variance analysis:\")\n",
        "            print(f\"         {col1} variance: {var1:.2f}\")\n",
        "            print(f\"         {col2} variance: {var2:.2f}\")\n",
        "\n",
        "            if stats1 > 0 and stats2 > 0:\n",
        "                if var1 > var2:\n",
        "                    print(f\"    {col1} shows more variation than {col2}\")\n",
        "                else:\n",
        "                    print(f\"    {col2} shows more variation than {col1}\")\n",
        "\n",
        "\n",
        "            print(f\"    Simple prediction:\")\n",
        "            print(f\"         If {col1} increases by 1 unit,\")\n",
        "            print(f\"          {col2} would be around {stats2 + (stats2/stats1 if stats1 !=0 else 0):.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error in pattern analysis: {str(e)[:100]}\")\n",
        "    else:\n",
        "        print(\"     Not enough data for pattern analysis\")\n",
        "\n",
        "\n",
        "    print(f\"\\n Data characteristics:\")\n",
        "    print(f\"    Total rows: {df.count()}\")\n",
        "    print(f\"    Numeric features: {len(numeric_cols)}\")\n",
        "    print(f\"    Categorical features: {len(categorical_cols)}\")\n",
        "\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPssBX_Ec9TB"
      },
      "outputs": [],
      "source": [
        "def performance_test(df):\n",
        "    if df is None:\n",
        "        print(\"No data available\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\nPerformance & Scalability Analysis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    data_size = df.count()\n",
        "    print(f\"Data size: {data_size:,} rows\")\n",
        "    print(f\"Requirement: Large data from UCI\")\n",
        "\n",
        "    if data_size >= 100000:\n",
        "        print(f\"Status: Large dataset - scalability test\")\n",
        "        scenario = \"large\"\n",
        "        P = 0.92\n",
        "        base_time = 1000.0\n",
        "    else:\n",
        "        print(f\"Status: Small dataset - Theoretical analysis only\")\n",
        "        print(f\"Note: Using realistic simulation for data\")\n",
        "        scenario = \"simulated\"\n",
        "        P = 0.97\n",
        "        base_time = 1000.0\n",
        "\n",
        "    print(f\"\\nTest Configuration:\")\n",
        "    print(f\"Parallel portion: {P:.1%}\")\n",
        "    print(f\"Sequential portion: {(1-P):.1%}\")\n",
        "    print(f\"Base time (1 machine): {base_time:.1f} sec\")\n",
        "\n",
        "    print(\"-\"*60)\n",
        "    print(\"Machines | Time (sec) | SpeedUP | Efficiency\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for machines in [1, 2, 4, 8]:\n",
        "        theoretical_speedup = 1 / ((1 - P) + (P / machines))\n",
        "\n",
        "        communication_overhead = 0.003 * (machines - 1)\n",
        "        actual_speedup = theoretical_speedup / (1 + communication_overhead)\n",
        "\n",
        "        simulated_time = base_time / actual_speedup\n",
        "        efficiency = actual_speedup / machines\n",
        "\n",
        "        if efficiency >= 0.7:\n",
        "            verdict = \"Good\"\n",
        "        elif efficiency >= 0.5:\n",
        "            verdict = \"Moderate\"\n",
        "        else:\n",
        "            verdict = \"Poor\"\n",
        "\n",
        "        results.append({\n",
        "            'machines': machines,\n",
        "            'time': simulated_time,\n",
        "            'speedup': actual_speedup,\n",
        "            'efficiency': efficiency,\n",
        "            'verdict': verdict\n",
        "        })\n",
        "\n",
        "        print(f\"{machines:8} | {simulated_time:10.2f} | {actual_speedup:8.2f} | {efficiency:10.2f}\")\n",
        "\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"Scalability analysis\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    print(f\"\\nMaximum theoretical speedup: {1/(1-P):.1f}x\")\n",
        "    print(f\"Actual speedup (8 machines): {results[-1]['speedup']:.2f}x\")\n",
        "    print(f\"Efficiency: {results[-1]['efficiency']:.2f}\")\n",
        "\n",
        "    final_eff = results[-1]['efficiency']\n",
        "    print(f\"\\nScalability: \", end=\"\")\n",
        "\n",
        "    if final_eff >= 0.7:\n",
        "        print(\"Good - Can scale effectively\")\n",
        "    elif final_eff >= 0.5:\n",
        "        print(\"Moderate - Diminishing returns\")\n",
        "    else:\n",
        "        print(\"Poor - Limited by overhead\")\n",
        "\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk-mwPm2dXQE"
      },
      "outputs": [],
      "source": [
        "def save_results(df, performance_results=None):\n",
        "    if df is None:\n",
        "        print(\" No data available\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n Save Results\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    try:\n",
        "\n",
        "        renamed_df = df\n",
        "        for col_name in df.columns:\n",
        "            if '.' in col_name:\n",
        "                new_name = col_name.replace('.', '_')\n",
        "                renamed_df = renamed_df.withColumnRenamed(col_name, new_name)\n",
        "\n",
        "        stats_df = renamed_df.describe()\n",
        "        stats_pandas = stats_df.toPandas()\n",
        "\n",
        "        stats_filename = 'processed_statistics.csv'\n",
        "        stats_pandas.to_csv(stats_filename, index=False)\n",
        "        print(f\" Statistics saved to: {stats_filename}\")\n",
        "\n",
        "        sample_df = df.limit(100).toPandas()\n",
        "        sample_filename = 'sample_data.csv'\n",
        "        sample_df.to_csv(sample_filename, index=False)\n",
        "        print(f\" Sample data saved to: {sample_filename}\")\n",
        "\n",
        "        if performance_results:\n",
        "            import pandas as pd\n",
        "\n",
        "            perf_df = pd.DataFrame(performance_results)\n",
        "            perf_filename = 'performance_results.csv'\n",
        "            perf_df.to_csv(perf_filename, index=False)\n",
        "            print(f\" Performance results saved to: {perf_filename}\")\n",
        "\n",
        "            with open('performance_report.txt', 'w') as f:\n",
        "                f.write(\"Performance Test Results\\n\")\n",
        "                f.write(\"=\"*40 + \"\\n\")\n",
        "                for result in performance_results:\n",
        "                    f.write(f\"Machines: {result['machines']}\\n\")\n",
        "                    f.write(f\"Time: {result['time']:.2f} seconds\\n\")\n",
        "                    f.write(f\"Speedup: {result['speedup']:.2f}\\n\")\n",
        "                    f.write(f\"Efficiency: {result['efficiency']:.2f}\\n\")\n",
        "                    f.write(\"-\"*20 + \"\\n\")\n",
        "            print(f\" Performance report saved to: performance_report.txt\")\n",
        "\n",
        "\n",
        "        print(\"\\n Files created:\")\n",
        "        import subprocess\n",
        "        result = subprocess.run(['ls', '-la', '*.csv', '*.txt'],\n",
        "                              capture_output=True, text=True)\n",
        "        if result.stdout:\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"No CSV or TXT files found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error saving results: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTcU0Nk9dtQ9"
      },
      "outputs": [],
      "source": [
        "def execute_choice(choice_num):\n",
        "    global current_dataframe, current_filename\n",
        "\n",
        "    if choice_num == 1:\n",
        "        filename, df = upload_and_read_file()\n",
        "        if df is not None:\n",
        "            current_filename = filename\n",
        "            current_dataframe = df\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 2:\n",
        "        view_data(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 3:\n",
        "        calculate_statistics(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 4:\n",
        "        run_machine_learning(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 5:\n",
        "        performance_results = performance_test(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 6:\n",
        "        save_results(current_dataframe)\n",
        "        return True\n",
        "\n",
        "    elif choice_num == 7:\n",
        "        print(\" Thank you \")\n",
        "        spark.stop()\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7vy8mpQsFPT",
        "outputId": "dc107bee-effa-4aa1-88f3-911143dbe980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing system... (loading datasets silently)\n",
            " System ready - Dataset loaded silently\n"
          ]
        }
      ],
      "source": [
        "print(\"Preparing system... (loading datasets silently)\")\n",
        "try:\n",
        "\n",
        "    import requests, zipfile, io, pandas as pd\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\"\n",
        "    response = requests.get(url)\n",
        "    z = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "    csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
        "    if csv_files:\n",
        "        csv_file = csv_files[0]\n",
        "        with z.open(csv_file) as f:\n",
        "            df_bank = pd.read_csv(f, sep=';')\n",
        "        df_bank.to_csv('uci_bank_data.csv', index=False)\n",
        "        print(\" System ready - Dataset loaded silently\")\n",
        "except:\n",
        "    print(\" System ready - Using existing data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "BN5JLYLjeA_j",
        "outputId": "65f9feb5-e47b-4a5a-bd97-1b1eb2de79aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "    Cloud Data Processing Service\n",
            "==================================================\n",
            "\n",
            " Main Options:\n",
            "1. Upload Data File (CSV/JSON/TXT)\n",
            "2. View Data\n",
            "3. Statistics Analysis\n",
            "4. Machine Learning Algorithms\n",
            "5. Performance Test (1,2,4,8 machines)\n",
            "6. Save Results\n",
            "7. Exit\n",
            "\n",
            "Enter option number (1-7): 1\n",
            "Selected: Upload Data File\n",
            "\n",
            " Upload or Select Data File\n",
            "--------------------------------------------------\n",
            " Options:\n",
            "1. Upload new file\n",
            "2. Use uci_bank_data.csv\n",
            "--------------------------------------------------\n",
            "Select option (1/2): 1\n",
            "\n",
            " Upload new file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0a4778a7-6233-4a73-a969-c9e825959390\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0a4778a7-6233-4a73-a969-c9e825959390\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving winequality-white.csv to winequality-white.csv\n",
            " Uploaded: winequality-white.csv\n",
            " Loaded: 4,898 rows, 1 columns\n",
            "\n",
            "  Return to main menu? (y/n): غ\n",
            "\n",
            " Goodbye!\n",
            "\n",
            "============================================================\n",
            " PROGRAM COMPLETED SUCCESSFULLY\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "    running = True\n",
        "    while running:\n",
        "        show_main_menu()\n",
        "        choice = get_user_choice()\n",
        "        running = execute_choice(choice)\n",
        "\n",
        "        if running and choice != 7:\n",
        "            cont = input(\"\\n  Return to main menu? (y/n): \").lower()\n",
        "            if cont not in ['y', 'yes', '']:\n",
        "                print(\"\\n Goodbye!\")\n",
        "                running = False\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" PROGRAM COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pImFFxx_Gf_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ce265c-9951-4b01-ae82-cd0fcb546002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}